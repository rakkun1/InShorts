# -*- coding: utf-8 -*-
"""AP project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MXBS7H1XKxOPgOWJbZKu1p7ojhW3dnK5
"""

import re
import heapq
import requests
import numpy as np
from gensim.models import KeyedVectors
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from bs4 import BeautifulSoup

import gensim.downloader as api
#from GoogleNews import GoogleNews


stopwords = stopwords.words("english")

def loadEmbeddingModel():
    loading = True
    tries = 0
    print("Loading pre-trained embedding model...")

    while loading:
        try:
            tries = tries + 1
            w2v_model = api.load("word2vec-google-news-300")
            loading = False
            print("Loading complete.")
        except Exception as ConnectionResetError:
            if tries <= 5:
                print('\nFailed:', ConnectionResetError)
                print('\nTrying again...\n')
            else:
                print('\nExecution terminated with error:', ConnectionResetError)
    return w2v_model

"""def getLinks(query, num_links=3):
    googlenews = GoogleNews(lang="en")
    googlenews.search(query)
    return googlenews.get_links()[:num_links]"""

def fetch_and_preprocess_text_from_urls(url_list):
    processed_texts = []

    for url in url_list:
        try:
            response = requests.get(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')

                # Extract text from HTML content
                extracted_text = soup.get_text()

                # Preprocess the text
                # Remove HTML tags
                extracted_text = re.sub(r'<[^>]+>', ' ', extracted_text)
                # Remove special characters and extra spaces
                extracted_text = re.sub(r"[^\w\s]", ' ', extracted_text)
                extracted_text = re.sub(r"\s+", ' ', extracted_text)

                # Append the preprocessed text to the list
                processed_texts.append(extracted_text)
            else:
                processed_texts.append("Error: Unable to fetch content from the URL")

        except Exception as e:
            processed_texts.append(f"Error: {str(e)}")

    return processed_texts

def cosineSimilarity(A, B):
    return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))

def merge(documents, w2v_model, threshold = 0.85):

    def get_custom_wv(word):
        try:
            return w2v_model.get_vector(word)
        except:
            return np.zeros(w2v_model.vector_size)

    documents_sentences = list(map(sent_tokenize, documents))
    largest_document = max(documents_sentences, key=len)
    final_document = largest_document
    for document in documents_sentences:
        if document == largest_document:
            continue
        for document_line_position, document_line in enumerate(document):
            position = list()
            for final_document_line_position, final_document_line in enumerate(final_document):
                document_line_vector = np.mean(
                    [get_custom_wv(word) for word in document_line.split()], axis=0)
                final_document_line_vector = np.mean(
                    [get_custom_wv(word) for word in final_document_line.split()], axis=0)
                similarity = cosineSimilarity(
                    document_line_vector, final_document_line_vector)
                position.append((final_document_line_position, similarity))
            position.sort(reverse=True, key=lambda x: x[1])
            best_position, highest_similarity = position[0]
            if highest_similarity >= threshold:
                final_document.insert(best_position, document_line)
    return " ".join(final_document)

def sumy_summarize(corpus, n):
    text = corpus
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = LsaSummarizer()
    summary = summarizer(parser.document, n)
    return summary

def summarize(corpus, mode='rank', ratio=0.5, num_sentences=7):
    if mode == "frequency":
        sentence_list = sent_tokenize(corpus)
        formatted_article_text = corpus
        word_frequencies = dict()
        for word in word_tokenize(formatted_article_text):
            if word not in stopwords:
                if word not in word_frequencies.keys():
                    word_frequencies[word] = 1
                else:
                    word_frequencies[word] += 1
        maximum_frequncy = max(word_frequencies.values())

        for word in word_frequencies.keys():
            word_frequencies[word] = (word_frequencies[word] / maximum_frequncy)

        sentence_scores = dict()
        for sent in sentence_list:
            words = word_tokenize(sent.lower())
            count_words = len(words)
            for word in words:
                if word in word_frequencies.keys():
                    if count_words < 50:
                        if sent not in sentence_scores.keys():
                            sentence_scores[sent] = word_frequencies[word]
                        else:
                            sentence_scores[sent] += word_frequencies[word]
        summary_sentences = heapq.nlargest(
            num_sentences, sentence_scores, key=sentence_scores.get)
        summary = " ".join(summary_sentences)
        return summary
    else:
        return sumy_summarize(corpus, num_sentences)

# Example usage:
print("Top Stories:\n1. Nepal Earthquake\n2. Israel War\n3. Khalistan threat")
urls=[["https://edition.cnn.com/2023/11/03/asia/nepal-earthquake-northwest-hnk-intl/index.html","https://www.bbc.com/news/world-asia-67317442"],["https://www.aljazeera.com/news/2023/11/4/extremely-challenging-israels-gaza-ground-assault-faces-stumbles","https://www.hindustantimes.com/world-news/israel-hamas-war-news-live-updates-5-nov-2023-gaza-conflict-palestine-humanitarian-pause-biden-blinken-101699142382568.html"],["https://www.indiatoday.in/india/story/khalistan-terrorist-gurpatwant-singh-pannun-video-air-india-flight-threat-2458111-2023-11-04","https://www.deccanherald.com/india/khalistan-terrorist-gurpatwant-singh-pannun-asks-sikhs-to-avoid-air-india-flight-on-nov-19-cites-danger-2757566"]]
query = int(input("Enter your pick to read: "))
if(query==1):
  result = fetch_and_preprocess_text_from_urls(urls[0])
elif(query==2):
  result = fetch_and_preprocess_text_from_urls(urls[1])
elif(query==3):
  result = fetch_and_preprocess_text_from_urls(urls[2])
else:
  print("Invalid selection")
  exit()

w2v_model = loadEmbeddingModel()
merged_article =merge(result, w2v_model)

summary =summarize(merged_article)
print(f"News on {query}:\n{summary}\n")